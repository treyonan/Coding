{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd90e23",
   "metadata": {},
   "source": [
    "Source codes for Python Machine Learning By Example 4th Edition (Packt Publishing)\n",
    "\n",
    "Chapter 13 Advancing language understanding and Generation with the Transformer models\n",
    "\n",
    "Author: Yuxi (Hayden) Liu (yuxi.liu.ece@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4276d493-7461-4472-a96f-a090dce388ca",
   "metadata": {},
   "source": [
    "# Generating text using GPT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10764a21",
   "metadata": {},
   "source": [
    "## Writing your own War and Peace with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5001b57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I love machine learning, so you should use machine learning as your tool for data production.\\n\\n'},\n",
       " {'generated_text': 'I love machine learning. I love learning and I love algorithms. I love learning to control systems.'},\n",
       " {'generated_text': 'I love machine learning, but it would be pretty difficult for it to keep up with the demands and'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(0)\n",
    "generator(\"I love machine learning\",\n",
    "          max_length=20,\n",
    "          num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b45af67-bba5-4b66-a96a-080e4a2b3e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, GPT2Tokenizer\n",
    "\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd2d5a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oreo/anaconda3/lib/python3.11/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text_dataset = TextDataset(tokenizer=tokenizer, file_path='warpeace_input.txt', block_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f215023a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6176"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca73c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5afef25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc1b85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06b2bb97-1572-47fa-a0dc-512df464a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./gpt_results', \n",
    "    num_train_epochs=20,     \n",
    "    per_device_train_batch_size=16, \n",
    "    logging_dir='./gpt_logs',\n",
    "    save_total_limit=1,\n",
    "    logging_steps=500,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d806642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=text_dataset,\n",
    "    optimizers=(optim, None)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b2eb622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7720' max='7720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7720/7720 23:31, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.414100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.149500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.882600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.779200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.699200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.548800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.495400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.447600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.401400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.367600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.335500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.315100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.300500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7720, training_loss=2.6408239769812076, metrics={'train_runtime': 1411.9948, 'train_samples_per_second': 87.479, 'train_steps_per_second': 5.467, 'total_flos': 8068697948160000.0, 'train_loss': 2.6408239769812076, 'epoch': 20.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69854b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt_text, model, tokenizer, max_length):\n",
    "    input_ids = tokenizer.encode(prompt_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate response\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    # Decode the generated responses\n",
    "    responses = []\n",
    "    for response_id in output_sequences:\n",
    "        response = tokenizer.decode(response_id, skip_special_tokens=True)\n",
    "        responses.append(response)\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9828b8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oreo/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the emperor's, and the Emperor Francis, who was in attendance on him,\n",
      "was present.\n",
      "\n",
      "The Emperor was present because he had received the news that the French\n",
      "troops were advancing on Moscow, that Kutuzov had been wounded, the\n",
      "Emperor's wife had died, a letter from Prince Andrew had come from\n",
      "Prince Vasili, Prince Bolkonski had seen at the palace, news of the death of\n",
      "the Emperor, but the most important news was that\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"the emperor\"\n",
    "responses = generate_text(prompt_text, model, tokenizer, 100)\n",
    "\n",
    "for response in responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f42bfa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e492d",
   "metadata": {},
   "source": [
    "Readers may ignore the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a60266f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[NbConvertApp] Converting notebook ch13_part2.ipynb to python\n",
      "[NbConvertApp] Writing 2580 bytes to ch13_part2.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to python ch13_part2.ipynb --TemplateExporter.exclude_input_prompt=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
