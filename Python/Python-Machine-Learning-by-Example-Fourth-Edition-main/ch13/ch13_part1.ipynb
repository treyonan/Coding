{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd90e23",
   "metadata": {},
   "source": [
    "Source codes for Python Machine Learning By Example 4th Edition (Packt Publishing)\n",
    "\n",
    "Chapter 13 Advancing language understanding and Generation with the Transformer models\n",
    "\n",
    "Author: Yuxi (Hayden) Liu (yuxi.liu.ece@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4276d493-7461-4472-a96f-a090dce388ca",
   "metadata": {},
   "source": [
    "# Understanding self-attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca73c4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 8, 1, 6, 2])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sentence = torch.tensor(\n",
    "    [0, # python \n",
    "     8, # machine      \n",
    "     1, # learning \n",
    "     6, # by \n",
    "     2] # example \n",
    ")\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a3fcc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "embed = torch.nn.Embedding(10, 16)\n",
    "sentence_embed = embed(sentence).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d132852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,\n",
       "          0.3223, -1.2633,  0.3500,  0.3081,  0.1198,  1.2377,  1.1168, -0.2473],\n",
       "        [-0.8834, -0.4189, -0.8048,  0.5656,  0.6104,  0.4669,  1.9507, -1.0631,\n",
       "         -0.0773,  0.1164, -0.5940, -1.2439, -0.1021, -1.0335, -0.3126,  0.2458],\n",
       "        [-1.3527, -1.6959,  0.5667,  0.7935,  0.5988, -1.5551, -0.3414,  1.8530,\n",
       "          0.7502, -0.5855, -0.1734,  0.1835,  1.3894,  1.5863,  0.9463, -0.8437],\n",
       "        [ 1.6459, -1.3602,  0.3446,  0.5199, -2.6133, -1.6965, -0.2282,  0.2800,\n",
       "          0.2469,  0.0769,  0.3380,  0.4544,  0.4569, -0.8654,  0.7813, -0.9268],\n",
       "        [-0.6136,  0.0316, -0.4927,  0.2484,  0.4397,  0.1124,  0.6408,  0.4412,\n",
       "         -0.1023,  0.7924, -0.2897,  0.0525,  0.5229,  2.3022, -1.4689, -1.5867]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6598cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = sentence_embed.shape[1]\n",
    "w_key = torch.rand(d, d)\n",
    "w_query = torch.rand(d, d)\n",
    "w_value = torch.rand(d, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "135f2144",
   "metadata": {},
   "outputs": [],
   "source": [
    "token1_embed = sentence_embed[0]\n",
    "key_1 = w_key.matmul(token1_embed)\n",
    "query_1 = w_query.matmul(token1_embed)\n",
    "value_1 = w_value.matmul(token1_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e5b88d-d79a-403a-869d-28e33943341b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1371, -0.5677, -0.9324, -0.3195, -2.8886, -1.2679, -1.1153,  0.2904,\n",
       "         0.3825,  0.3179, -0.4977, -3.8230,  0.3699, -0.3932, -1.8788, -3.3556])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "292b5a7b-8b8d-4f10-abd8-57b1a4b631a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = sentence_embed.matmul(w_key.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96556dd7-ab21-45c5-9898-053957d5c8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1371, -0.5677, -0.9324, -0.3195, -2.8886, -1.2679, -1.1153,  0.2904,\n",
       "         0.3825,  0.3179, -0.4977, -3.8230,  0.3699, -0.3932, -1.8788, -3.3556])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb1ca2e9-bf1f-4298-8689-dd32b5fa51b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = sentence_embed.matmul(w_value.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bff4e7a-9ea6-4939-be1f-9b60f1870302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "a1 = F.softmax(query_1.matmul(keys.T) / d ** 0.5, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84b3f2c6-0f2c-4a78-9b7c-cd9caf77449c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.2481e-01, 4.2515e-01, 6.8915e-06, 2.5002e-01, 1.5529e-05])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20edbfc3-a5ea-49bd-8933-70f7997986f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7136, -1.1795, -0.5726, -0.4959, -0.6838, -1.6460, -0.3782, -1.0066,\n",
       "        -0.4798, -0.8996, -1.2138, -0.3955, -1.3302, -0.3832, -0.8446, -0.8470])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = a1.matmul(values)\n",
    "z1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4949e1f4-f64f-4248-b469-62bb922391cd",
   "metadata": {},
   "source": [
    "# Improving sentiment analysis with BERT and Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d68954-a57f-479a-88b2-775242ec7ae3",
   "metadata": {},
   "source": [
    "## Fine-tuning a pre-trained BERT model for sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd6115c1-a60c-4751-904b-b3aba22d7dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_dataset = list(IMDB(split='train'))\n",
    "test_dataset = list(IMDB(split='test'))\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5665c852-566e-4b17-8912-f7219ea43fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [train_sample[1] for train_sample in train_dataset]\n",
    "train_labels = [train_sample[0] for train_sample in train_dataset]\n",
    "\n",
    "test_texts = [test_sample[1] for test_sample in test_dataset]\n",
    "test_labels = [test_sample[0] for test_sample in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b45af67-bba5-4b66-a96a-080e4a2b3e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "756e31c4-fc8a-4284-80c8-4de011574755",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c26a0d50-d382-46e3-a5b1-073b1a3fa869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8544a99f-3821-4269-9cda-f84ca8d86d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor([0., 1.] if self.labels[idx] == 2 else [1., 0.])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4662bf04-fd95-4609-a0e0-cc41226b136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "test_encoded_dataset = IMDbDataset(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "daa4f0e2-8d58-4523-ad7d-784276c16c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dl = torch.utils.data.DataLoader(train_encoded_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dl = torch.utils.data.DataLoader(test_encoded_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5f0799f-1ccc-4a11-83c2-c8da893ae007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', local_files_only=True)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63cdc04e-46fa-4f09-b56f-85eaff558236",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c854f8e5-f647-4d4f-be18-809468752fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs['loss'] \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()*len(batch)\n",
    "\n",
    "    return total_loss/len(dataloader.dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fc60173-0c1d-476e-98fa-115ef22e0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "\n",
    "            pred = torch.argmax(logits, 1)\n",
    "            total_acc += (pred == torch.argmax(labels, 1)).float().sum().item()\n",
    "\n",
    "    return  total_acc/len(dataloader.dataset)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4acedf5-acbd-4903-98d9-94f75491e126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - loss: 0.0244 - accuracy: 0.9646\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "num_epochs = 1 \n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_dl, optimizer)\n",
    "    train_acc = evaluate(model, train_dl)\n",
    "    print(f'Epoch {epoch+1} - loss: {train_loss:.4f} - accuracy: {train_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd300520-c729-459a-b9db-1f28f4e4cc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 92.96 %\n"
     ]
    }
   ],
   "source": [
    "test_acc = evaluate(model, test_dl)\n",
    "print(f'Accuracy on test set: {100 * test_acc:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7aaa93da-beea-47cb-9d88-6cebef5a647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.mem_get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62df971f-a829-4244-b274-370487c7498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6f9c66e-e0b4-44bc-bcf6-d3abceb36888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up memory\n",
    "del model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e178193",
   "metadata": {},
   "source": [
    "## Using the Trainer API to train Transformer models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45a46954-3928-45e8-830e-a76e761f2864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', local_files_only=True)\n",
    "model.to(device)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d673d6cf-9a53-456b-98ae-4315c302910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge accelerate -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06b2bb97-1572-47fa-a0dc-512df464a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results', \n",
    "    num_train_epochs=1,     \n",
    "    per_device_train_batch_size=32, \n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d806642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_encoded_dataset,\n",
    "#     optimizers=(optim, None)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11958080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33302/2881970286.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred  \n",
    "    pred = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=pred, references=np.argmax(labels, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abf67d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    compute_metrics=compute_metrics,\n",
    "    args=training_args,\n",
    "    train_dataset=train_encoded_dataset,\n",
    "    eval_dataset=test_encoded_dataset,\n",
    "    optimizers=(optim, None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54ce84b5-2a67-4a9e-b8fa-e9cde6bd637e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 06:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.446900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.317100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.329800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.262900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.257300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.242300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.234200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.225800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.212400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.228200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.204800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.198000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=782, training_loss=0.2543304844585526, metrics={'train_runtime': 377.3168, 'train_samples_per_second': 66.257, 'train_steps_per_second': 2.073, 'total_flos': 3311684966400000.0, 'train_loss': 0.2543304844585526, 'epoch': 1.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb281451-46c2-41b6-8282-f92ac20801b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18691246211528778, 'eval_accuracy': 0.9292, 'eval_runtime': 123.198, 'eval_samples_per_second': 202.925, 'eval_steps_per_second': 25.366, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(trainer.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e8197",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8782038",
   "metadata": {},
   "source": [
    "Readers may ignore the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3485aea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[NbConvertApp] Converting notebook ch13_part1.ipynb to python\n",
      "[NbConvertApp] Writing 6092 bytes to ch13_part1.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to python ch13_part1.ipynb --TemplateExporter.exclude_input_prompt=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
