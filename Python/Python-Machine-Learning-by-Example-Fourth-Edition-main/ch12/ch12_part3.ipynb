{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5531ec-f646-4d7e-97b8-ead047c7b3ad",
   "metadata": {},
   "source": [
    "Source codes for Python Machine Learning By Example 4th Edition (Packt Publishing)\n",
    "\n",
    "Chapter 12 Making Predictions with Sequences Using Recurrent Neural Networks\n",
    "\n",
    "Author: Yuxi (Hayden) Liu (yuxi.liu.ece@gmail.com)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9009fbe-9628-4518-9960-ef8eb81d04bd",
   "metadata": {},
   "source": [
    "# Writing your own War and Peace with RNNs "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc0a4b24-7b74-463d-8cf3-f6e8508fecde",
   "metadata": {},
   "source": [
    "## Acquiring and analyzing the training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b5486a-6e27-419c-9a79-3b96062a2b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('warpeace_input.txt', 'r', encoding=\"utf8\") as fp:\n",
    "    raw_text = fp.read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c72d1ebf-d8b3-46a6-9afc-c99edd77e820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\"well, prince, so genoa and lucca are now just family estates of the\n",
      "buonapartes. but i warn you, if you don't tell me that this means war,\n",
      "if you still try to defend the infamies and horrors perpetr\n"
     ]
    }
   ],
   "source": [
    "print(raw_text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aab42fad-ad53-4f37-bc07-c372627dd2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 39830\n"
     ]
    }
   ],
   "source": [
    "all_words = raw_text.split()\n",
    "unique_words = list(set(all_words))\n",
    "print(f'Number of unique words: {len(unique_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7b89e16-a1cd-450a-8bef-e01126e05fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 3196213\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "print(f'Total characters: {n_chars}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab8160aa-4570-42ec-b23e-41a6d5549eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary (unique characters): 57\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'ä', 'é', 'ê', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "vocab_size = len(chars)\n",
    "print(f'Total vocabulary (unique characters): {vocab_size}')\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e4f363-d427-4d74-bf44-30fe35872ce0",
   "metadata": {},
   "source": [
    "## Constructing the training set for the RNN text generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "563a8653-bd3c-40a8-9da2-6cf411464249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, '*': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '=': 24, '?': 25, 'a': 26, 'b': 27, 'c': 28, 'd': 29, 'e': 30, 'f': 31, 'g': 32, 'h': 33, 'i': 34, 'j': 35, 'k': 36, 'l': 37, 'm': 38, 'n': 39, 'o': 40, 'p': 41, 'q': 42, 'r': 43, 's': 44, 't': 45, 'u': 46, 'v': 47, 'w': 48, 'x': 49, 'y': 50, 'z': 51, 'à': 52, 'ä': 53, 'é': 54, 'ê': 55, '\\ufeff': 56}\n"
     ]
    }
   ],
   "source": [
    "index_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "char_to_index = dict((c, i) for i, c in enumerate(chars))\n",
    "print(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bee0a0bb-78ca-437b-9d59-2a310e4cf2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "text_encoded = np.array(\n",
    "    [char_to_index[ch] for ch in raw_text],\n",
    "    dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99664e4e-6616-437f-8ab8-ef2f9fd5b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "\n",
    "text_chunks = np.array([text_encoded[i:i+chunk_size] \n",
    "               for i in range(len(text_encoded)-chunk_size+1)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bdec993-0f3e-4ea0-b517-efacc5cf222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text_chunk = self.text_chunks[idx]\n",
    "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
    "    \n",
    "seq_dataset = SeqDataset(torch.from_numpy(text_chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "463d815f-87f9-4064-b294-444e54a58d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    " \n",
    "batch_size = 64\n",
    "\n",
    "torch.manual_seed(0)\n",
    "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d617b211-05b7-490e-af1d-1d83216a68ca",
   "metadata": {},
   "source": [
    "## Building and Training an RNN text generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6533b03a-8b3f-43b2-a4e6-20860dc70809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) \n",
    "        self.rnn_hidden_dim = rnn_hidden_dim\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_dim, \n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x).unsqueeze(1)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_dim)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_dim)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db072efb-0fae-446e-ae2b-82e3ea18059b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(57, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=57, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "rnn_hidden_dim = 512\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_dim) \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b59be30-7631-47a3-b020-a0203cd0717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6db22e5d-e0b2-4344-b289-77257cb93af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 4.0255\n",
      "Epoch 500 - loss: 1.4560\n",
      "Epoch 1000 - loss: 1.2794\n",
      "Epoch 1500 - loss: 1.3793\n",
      "Epoch 2000 - loss: 1.3275\n",
      "Epoch 2500 - loss: 1.3033\n",
      "Epoch 3000 - loss: 1.2388\n",
      "Epoch 3500 - loss: 1.2926\n",
      "Epoch 4000 - loss: 1.2658\n",
      "Epoch 4500 - loss: 1.2186\n",
      "Epoch 5000 - loss: 1.2181\n",
      "Epoch 5500 - loss: 1.2342\n",
      "Epoch 6000 - loss: 1.2134\n",
      "Epoch 6500 - loss: 1.2532\n",
      "Epoch 7000 - loss: 1.2642\n",
      "Epoch 7500 - loss: 1.2028\n",
      "Epoch 8000 - loss: 1.2410\n",
      "Epoch 8500 - loss: 1.2557\n",
      "Epoch 9000 - loss: 1.2014\n",
      "Epoch 9500 - loss: 1.2442\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    hidden, cell = model.init_hidden(batch_size)\n",
    "    seq_batch, target_batch = next(iter(seq_dl))\n",
    "    seq_batch = seq_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    for c in range(seq_length):\n",
    "        pred, hidden, cell = model(seq_batch[:, c], hidden.to(device), cell.to(device)) \n",
    "        loss += loss_fn(pred, target_batch[:, c])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss = loss.item()/seq_length\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch {epoch} - loss: {loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fdd5214-6022-4b55-a693-bce2e58cac01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the emperor!\" said he.\n",
      "\n",
      "\"finished! it's all with moscow, it's not get bald hills!\" he added the civer with whom and desire to change. they really asked the imperor's field!\" she said. alpaty. there happed the cause of the longle matestood itself. \"the mercy tiresist between paying so impressions, and till the staff offsicilling petya, the chief dear body, returning quite dispatchma--he turned and ecstatically. \"ars doing her dome.\" said rostov, and the general feelings of the bottom would be the pickled ha\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "def generate_text(model, starting_str, len_generated_text=500):\n",
    "    encoded_input = torch.tensor([char_to_index[s] for s in starting_str])\n",
    "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
    "\n",
    "    generated_str = starting_str\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    hidden, cell = model.init_hidden(1)\n",
    "    for c in range(len(starting_str)-1):\n",
    "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell) \n",
    "    \n",
    "    last_char = encoded_input[:, -1]\n",
    "    for _ in range(len_generated_text):\n",
    "        logits, hidden, cell = model(last_char.view(1), hidden, cell) \n",
    "        logits = torch.squeeze(logits, 0)\n",
    "        last_char = Categorical(logits=logits).sample()\n",
    "        generated_str += str(index_to_char[last_char.item()])\n",
    "        \n",
    "    return generated_str\n",
    "\n",
    "\n",
    "model.to('cpu')\n",
    "torch.manual_seed(0)\n",
    "print(generate_text(model, 'the emperor', 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d207a-00a7-40a0-854c-7592e6976c63",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b70a9b-1da8-45a4-8229-44ee1c8f1b0a",
   "metadata": {},
   "source": [
    "Readers may ignore the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edc6c742-cb05-4d92-9ae3-66f0009af123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ch12_part3.ipynb to python\n",
      "[NbConvertApp] Writing 4748 bytes to ch12_part3.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to python ch12_part3.ipynb --TemplateExporter.exclude_input_prompt=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
